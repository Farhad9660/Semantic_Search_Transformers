{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec155d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of models\n",
    "models = ['sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "          'sentence-transformers/LaBSE', \n",
    "          'ahdsoft/persian-sentence-transformer-news-wiki-pairs-v3',\n",
    "          'AIDA-UPM/mstsb-paraphrase-multilingual-mpnet-base-v2',\n",
    "          'ViravirastSHZ/Hafez_Bert',\n",
    "          'HooshvareLab/bert-base-parsbert-uncased',\n",
    "          'pedramyazdipoor/persian_xlm_roberta_large',\n",
    "          'FacebookAI/xlm-roberta-large',\n",
    "          'Linq-AI-Research/Linq-Embed-Mistral',\n",
    "         \n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55689547",
   "metadata": {},
   "source": [
    "# Semantic Search Pipeline for Test Data using Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e167ed",
   "metadata": {},
   "source": [
    "### 1- Import Libraries and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f7308-efa8-48ac-9b21-a5c7a4ee43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c928a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import pickle\n",
    "# from tqdm.autonotebook import tqdm, trange\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# parameters \n",
    "dataset_path = 'Test_collections/'\n",
    "\n",
    "\n",
    "#model_folder = 'FT_models/'\n",
    "# models_path = 'D:/Amini/Dev_works/HF_models'\n",
    "# test data test\n",
    "dataset_name = 'Results_Pools_For_Queries_Mordad_1403_sorted.xlsx'\n",
    "\n",
    "\n",
    "# zero_shut test\n",
    "# dataset_name = 'Results_pool_for_ZERO_shut_Mordad_1403_sorted.xlsx'\n",
    "\n",
    "\n",
    "#hf access token\n",
    "write_access_token = '***'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348af233",
   "metadata": {},
   "source": [
    "### Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'PSTNWPv3'  # used for storing results\n",
    "#model_name = 'sentence-transformers/' + model\n",
    "model = 'Best_FT_models/ahdsoft/persian-sentence-transformer-news-wiki-pairs-v3'\n",
    "#model = 'myrkur/sentence-transformer-parsbert-fa'\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c44bfe",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defininng a function for comparison of strings\n",
    "def count_string_occurrences(list1, list2):\n",
    "    occurrences = 0\n",
    "    for string in list1:\n",
    "        if string in list2:\n",
    "            occurrences += 1\n",
    "    return  occurrences \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df4f1e",
   "metadata": {},
   "source": [
    "### 2- Loading the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca828bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(dataset_path  + dataset_name )\n",
    "query_set = dataset['query']\n",
    "\n",
    "\n",
    "# list of queries for search\n",
    "queries = query_set.unique().tolist()\n",
    "verses = dataset['verse'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fcc660-a8d2-4451-9952-35dfce52648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4881ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ca752",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86379af7",
   "metadata": {},
   "source": [
    "### 3- Perform Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24f868-bb83-47f6-b8a6-b77cbbd9c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "embedder = SentenceTransformer(model , trust_remote_code=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59feec5-70a6-47e9-924f-b024a29565f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the st model\n",
    "\n",
    "# Create embeddings of verses\n",
    "#verse_embeddings  = embedder.encode(verses )\n",
    "verse_embeddings  = embedder.encode(verses,  convert_to_tensor = True )\n",
    "\n",
    "# normalized embeddings\n",
    "#verse_embeddings.c\n",
    "# norms = np.linalg.norm(verse_embeddings, ord=2, axis=1, keepdims=True)\n",
    "# norms[norms == 0] = 1\n",
    "# verse_embeddings = verse_embeddings / norms\n",
    "\n",
    "\n",
    "# Create embeddings of queries\n",
    "query_embeddings = embedder.encode(queries,  convert_to_tensor = True)\n",
    "\n",
    "# normalized embeddings\n",
    "# norms = np.linalg.norm(query_embeddings, ord=2, axis=1, keepdims=True)\n",
    "# norms[norms == 0] = 1\n",
    "# query_embeddings = query_embeddings / norms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a96953-ab48-4280-a6d4-cf927419c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dictionary of similarity fucntions\n",
    "sim_function = {'cosine': util.cos_sim , 'dot': util.dot_score , 'euclidean': util.euclidean_sim , 'manhattan': util.manhattan_sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92650a88-9c0b-4fdf-9d62-cd416a2ff96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafdc4c1-0228-4526-80ab-3621f85dc7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b495d7-7fe5-4c82-99e7-7154fbb52492",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_fn = embedder.similarity_fn_name\n",
    "if sim_fn is None:\n",
    "    sim_fn = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45548302-013b-4436-ae3e-0c03e6fc4920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc199355-fd47-4b1d-99d8-34c0e4a0f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder.similarity_fn_name = 'cosine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad41e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Attempt to get the similarity function name from the model\n",
    "\n",
    "for top_k in [10 , 20]:\n",
    "        # Perform Semantic Search\n",
    "    print('the model similarity function is: ', sim_fn)\n",
    "    answers = util.semantic_search(query_embeddings, verse_embeddings, top_k = top_k, score_function = sim_function[sim_fn] )\n",
    "    \n",
    "    # creating a dataset from the search results\n",
    "    df = pd.DataFrame(columns = ['query', 'query_id', 'answer', 'search_score'])\n",
    "    df_list = []\n",
    "    for query in queries: \n",
    "    \n",
    "        df_temp = pd.DataFrame(answers[queries.index(query)])\n",
    "        df_temp['query_id'] = queries.index(query)+1\n",
    "        df_temp['query'] = query\n",
    "        \n",
    "        df_list.append(df_temp)\n",
    "    df = pd.concat(df_list)\n",
    "    \n",
    "    df = df.set_index('corpus_id')\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    df['answer'] = [verses[corpus_id] for corpus_id in df['corpus_id']]\n",
    "    search_results = df[['query_id', 'query', 'answer', 'corpus_id','score' ]]\n",
    "    \n",
    "    \n",
    "    # save results intro an excel file\n",
    "    search_results.to_excel(results_path + model_name + 'k_{}.xlsx'.format(top_k))\n",
    "    \n",
    "    ###  Evaluation\n",
    "    q_results = search_results[['query_id' , 'query', 'answer', 'score']]\n",
    "    gt = dataset[['query', 'verse', 'relevance score']]\n",
    "    \n",
    "    \n",
    "    # evaluation with comparison  - how many of the results happen in the ground truth verses\n",
    "    TPR = {} # true positive rate or recall\n",
    "    Precision = {}\n",
    "    NDCG = {} # Precesion\n",
    "    for q in queries:\n",
    "        q_result = q_results[q_results['query'] == q]['answer']\n",
    "        result_list = q_result.to_list()\n",
    "        gt_verses = gt[gt['query'] == q]['verse'] \n",
    "        gt_verses_list = gt_verses.tolist()\n",
    "        occurences = count_string_occurrences(result_list, gt_verses_list)\n",
    "        TPR[q] = occurences / len(gt_verses_list)  # it now recall\n",
    "        Precision[q] = occurences/ top_k #(we get the top 10)\n",
    "        \n",
    "    # calculating the third measure of performance  (NDCG)\n",
    "    for q in queries:\n",
    "        q_result =  q_results[q_results['query'] == q][['answer', 'score']]\n",
    "        gt_verses = gt[gt['query'] == q][['verse', 'relevance score']]\n",
    "        merged_df = pd.merge(q_result, gt_verses, left_on='answer', right_on='verse', how='left')\n",
    "        merged_df['relevance score'] = merged_df['relevance score'].fillna(0)\n",
    "        # Selecting only the columns of interest: 'answer', 'score'\n",
    "        matched_verses = merged_df[['answer', 'relevance score', 'score']]\n",
    "    \n",
    "        # Prepare the arrays for ndcg_score\n",
    "        true_relevance = matched_verses['relevance score'].to_numpy().reshape(1, -1)\n",
    "        predicted_scores = matched_verses['score'].to_numpy().reshape(1, -1)\n",
    "        # calculating the NDCG\n",
    "        from sklearn.metrics import ndcg_score\n",
    "        q_ndcg = ndcg_score(true_relevance, predicted_scores)\n",
    "    \n",
    "        NDCG[q] =  q_ndcg\n",
    "    \n",
    "\n",
    "    # save results intro an excel file\n",
    "    query_scores = pd.DataFrame( data = { 'Recall' : TPR.values(), 'Precision':Precision.values(), 'NDCG': NDCG.values()} , index = TPR.keys())\n",
    "    query_scores.reset_index(inplace=True)\n",
    "    \n",
    "    query_scores.rename(columns ={'index' : 'query'} , inplace = True)\n",
    "    \n",
    "    query_scores.loc[len(query_scores.index)] = ['mean', np.mean(query_scores['Recall']), np.mean(query_scores['Precision']), np.mean(query_scores['NDCG'])]\n",
    "    query_scores.to_excel(results_path + model_name + 'k_{}_scores.xlsx'.format(top_k))\n",
    "\n",
    "    print('The Testing process for the model {} is finished.\\n'.format(model_name))\n",
    "    print(' The recall @ k for k = {} is:{} '. format(top_k , np.mean(query_scores['Recall'])))\n",
    "    print(' The  P@K for top_k = {} is: {}'. format(top_k, np.mean(query_scores['Precision'])))    \n",
    "    print(' The NDCG @ k for k = {} is:  {}'. format(top_k , np.mean(query_scores['NDCG'])))\n",
    "    print ('****************************************************************************\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95b965-2923-4082-b45d-9894b715a23e",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
